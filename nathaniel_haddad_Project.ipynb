{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qE6Dme3N0Cb-"
   },
   "source": [
    "# Classifying Wikipedia Comments\n",
    "\n",
    "***\n",
    "\n",
    "Nathaniel Haddad - Northeastern University - 2019\n",
    "\n",
    "CS5100 - Foundation of Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Python 3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BoSIM0Ys0CcG"
   },
   "source": [
    "# Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9PFYC000CcJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from inspect import signature\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from google.colab import drive\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from spellchecker import SpellChecker\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-qXz6LCRE-G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /Users/nathanielhaddad/anaconda3/lib/python3.7/site-packages (0.5.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ec-rw6Jr0CcY"
   },
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHo3yXZV0CcR"
   },
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSc4oFw7EMe"
   },
   "source": [
    "Google Drive File Retrieval\n",
    "\n",
    "**NOTE:** Use this only if your path to files matches the path below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NvAEl4q0Qss"
   },
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZyusGf10zIg"
   },
   "outputs": [],
   "source": [
    "# comments = pd.read_csv('/content/drive/My Drive/cs5100 project/attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "# annotations = pd.read_csv('/content/drive/My Drive/cs5100 project/attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9OpSaP_zvAe"
   },
   "source": [
    "Normal File Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKmJnvmz0Cca"
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxvYdDxT0Ccg"
   },
   "outputs": [],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEx91J_80Ccn"
   },
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQZ5C6RQ0Cct"
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6tssOCo0Ccy"
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vl43L3vCjlFa"
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdFFfvNvrPHP"
   },
   "outputs": [],
   "source": [
    "comments.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfQSfG95N00x"
   },
   "source": [
    "**Remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zc2W5xPNLMUB"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tkTDM64MdSI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function: is_stopword\n",
    "param(s): word, a string\n",
    "returns: a boolean\n",
    "does: determines whether or not the given string is a stopword\n",
    "\"\"\"\n",
    "def is_stopword(word):\n",
    "  if word in STOPWORDS:\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwKnk0PiOpks"
   },
   "source": [
    "**Correct spelling**\n",
    "\n",
    "Does not improve ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyBF9mNOQf5F"
   },
   "outputs": [],
   "source": [
    "# spell = SpellChecker()\n",
    "\n",
    "# \"\"\"\n",
    "# function: correct_spelling\n",
    "# param(s): word, a string\n",
    "# returns: a string\n",
    "# does: corrects the spelling of a word using the spellchecker package\n",
    "# \"\"\"\n",
    "# def correct_spelling(word):\n",
    "#   misspelled_word = spell.unknown(word)\n",
    "#   if word is misspelled_word:\n",
    "#     return spell.correction(word)\n",
    "#   return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obKfhTZ4j-ar"
   },
   "source": [
    "Parse comments[\"comment\"] and clean text. \n",
    "\n",
    "\n",
    "**NOTE:** I tested all of the techniques used in this function individually, and none increased AUC values. This is interesting, as the vectorizers below perform many of the operations described here. Aside from spellchecking, each text cleanup technique is pretty standard, but may oversimplify the problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxcMi3dl7fGa"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function: parse_text\n",
    "param(s): text, a string\n",
    "returns: a string\n",
    "\"\"\"\n",
    "def parse_text(text):\n",
    "  new_text = []\n",
    "  # split text into list of items\n",
    "  words_and_symbols = str(text).split()\n",
    "  # iterate through each item and create a new string of alphabet characters\n",
    "  for item in words_and_symbols:\n",
    "    # make item lower case\n",
    "    item = item.lower()\n",
    "    # remove non-alpha characters\n",
    "    if item.isalpha():\n",
    "      # correct spelling\n",
    "      word = correct_spelling(item)\n",
    "      # remove stopwords\n",
    "      if not is_stopword(word):\n",
    "        new_text.append(word)\n",
    "  return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEAzHNQ7B7_6"
   },
   "outputs": [],
   "source": [
    "# After testing text cleaning results above, no need to use this\n",
    "# comments[\"comment\"] = comments[\"comment\"].apply(parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpdlQFrp0Cc3"
   },
   "outputs": [],
   "source": [
    "comments.query('attack')['comment'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5q03GMuhy-EG"
   },
   "source": [
    "# Metric Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXV_t7aFZmh6"
   },
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqDAHFb2YqsA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function: build_confusion_matrix\n",
    "params: model, a function\n",
    "returns: nothing\n",
    "does: builds and prints a confusion matrix\n",
    "\"\"\"\n",
    "def build_confusion_matrix(model, y_pred):\n",
    "  cm = confusion_matrix(y_pred, test_comments['attack'])\n",
    "  print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juZwQjmpy6UT"
   },
   "source": [
    "Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2EFgPnXeUYA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function: plot_precision_recall\n",
    "params: precision, a float; recall, a float\n",
    "returns: nothing\n",
    "does: plots precision as a function of recall\n",
    "source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "\"\"\"\n",
    "def plot_precision_recall(precision, recall):\n",
    "  print('post')\n",
    "  step_kwargs = ({'step': 'post'}\n",
    "                if 'step' in signature(plt.fill_between).parameters\n",
    "                else {})\n",
    "\n",
    "  plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "  print('post2')\n",
    "  plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "  print('post3')\n",
    "  plt.xlabel('Recall')\n",
    "  plt.ylabel('Precision')\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.title('2-class Precision-Recall curve:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95HYxdJmcTwX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function: precision_recall_fscore\n",
    "params: clf, a function\n",
    "returns: nothing\n",
    "does: calculates precision, recall, and f-score of given function\n",
    "\"\"\"\n",
    "def precision_recall_fscore(clf, y_pred):\n",
    "  metrics = precision_recall_fscore_support(y_true=test_comments['attack'], y_pred=y_pred, average='weighted')\n",
    "  print('Test Precision: %.5f' %metrics[0])\n",
    "  print('Test Recall: %.5f' %metrics[1])\n",
    "  print('Test F-Score: : %.5f' %metrics[2])\n",
    "  # precision_curve, recall_curve, _ = precision_recall_curve(test_comments['attack'], clf.predict(test_comments['comment']))\n",
    "  # plot_precision_recall(precision_curve, recall_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gg14rSn1yNuS"
   },
   "source": [
    "Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_8bF7qm0vJJ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function: get_metrics\n",
    "params: clf, a function\n",
    "returns: nothing\n",
    "does: prints out confusion matrix, precision, recall, f-score, and ROC AUC\n",
    "\"\"\"\n",
    "def get_metrics(clf):\n",
    "  y_pred = clf.predict(test_comments['comment'])\n",
    "  build_confusion_matrix(clf, y_pred)\n",
    "  precision_recall_fscore(clf, y_pred)\n",
    "  auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:,1])\n",
    "  print('Test ROC AUC: %.5f' %auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0eHjLAozGsx"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd1bChpct9rc"
   },
   "source": [
    "**Logistic Regression (strawman)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dnbBM6ra0Cc-"
   },
   "outputs": [],
   "source": [
    "# fit a simple text classifier\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "get_metrics(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJhzfjPV00MA"
   },
   "outputs": [],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2cdyGD_1Hfq"
   },
   "outputs": [],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GyMStbSedlAF"
   },
   "source": [
    "[[20280  1236]\n",
    " [  142  1520]]\n",
    "\n",
    "Test Precision: 0.93923\n",
    "\n",
    "Test Recall: 0.94055\n",
    "\n",
    "Test F-Score: : 0.93396\n",
    "\n",
    "Test ROC AUC: 0.95697"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCOKINlCbpa3"
   },
   "source": [
    "**Logistic Regression (\\#2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkWw9OxnzZXT"
   },
   "source": [
    "Create New Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYn-oz1dzWxt"
   },
   "outputs": [],
   "source": [
    "# create a new training set made up of validation set and previous training set\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "val_comments = comments.query(\"split=='dev'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "train_comments = pd.concat([val_comments, train_comments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XufJAIPGZwZp"
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    # replace CountVectorizer with TfidfVectorizer\n",
    "    ('vect', TfidfVectorizer(max_df=1.0, min_df=1, max_features=None, norm = 'l2')),\n",
    "    # leave in the Transformer for now\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LogisticRegression(n_jobs=-1)),\n",
    "])\n",
    "\n",
    "parameters = {'vect__lowercase': (True, False),\n",
    "              'vect__analyzer': ('word', 'char', 'char_wb'),\n",
    "              'vect__ngram_range': [(1,1), (1,2)],\n",
    "              'vect__stop_words': ('english', None),\n",
    "              'clf__solver': ('newton-cg', 'lbfgs')}\n",
    "\n",
    "# now using GridSearchCV for tuning\n",
    "clf = GridSearchCV(clf, parameters, cv=10, n_jobs=-1)\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "get_metrics(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQlj45810zdm"
   },
   "outputs": [],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oEtj8_x1Fhm"
   },
   "outputs": [],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Crt9FMkPem_n"
   },
   "source": [
    "[[20300  1325]\n",
    " [  122  1431]]\n",
    "\n",
    "Test Precision: 0.93667\n",
    "\n",
    "Test Recall: 0.93757\n",
    "\n",
    "Test F-Score: : 0.92975\n",
    "\n",
    "Test ROC AUC: 0.96079\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyWwx_NVhul2"
   },
   "source": [
    "**Logisitic Regression (\\#3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0ChFKoCzA4y"
   },
   "source": [
    "Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbsbRldRtAVX"
   },
   "outputs": [],
   "source": [
    "# from lecture\n",
    "vectorizerW = TfidfVectorizer(lowercase=True, analyzer='word', stop_words=None, ngram_range = (1,1), max_df=1.0, min_df=1, max_features=None, norm = 'l2')\n",
    "vectorizerC = TfidfVectorizer(lowercase=True, analyzer='char', stop_words=None, ngram_range = (1,1), max_df=1.0, min_df=1, max_features=None, norm = 'l2')\n",
    "# this variable will be used from now on\n",
    "combined_features = FeatureUnion([('word', vectorizerW), ('char', vectorizerC)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnX6v8togglW"
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    # new FeatureUnion implementation\n",
    "    ('features', combined_features),\n",
    "    ('clf', LogisticRegression(n_jobs=-1)),\n",
    "])\n",
    "\n",
    "parameters = {'clf__solver': ('newton-cg', 'lbfgs')}\n",
    "\n",
    "clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1)\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "get_metrics(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqVsRlyK0yIC"
   },
   "outputs": [],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lm6g-h0j1E5_"
   },
   "outputs": [],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7yE6MnIGq0H"
   },
   "source": [
    "[[20262  1152]\n",
    " [  160  1604]]\n",
    " \n",
    "Test Precision: 0.94182\n",
    "\n",
    "Test Recall: 0.94339\n",
    "\n",
    "Test F-Score: : 0.93785\n",
    "\n",
    "Test ROC AUC: 0.96259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FbzikbPb52a"
   },
   "source": [
    "**Logistic Regression (\\#4)**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "zs1vp-Lftw4_",
    "outputId": "82f6c179-0eb4-4498-9ca6-96e2303c1e21"
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('features', combined_features),\n",
    "    ('clf', LogisticRegressionCV(cv=10,max_iter=100, n_jobs=-1, solver='lbfgs', random_state=12345)),\n",
    "])\n",
    "\n",
    "parameters = {'clf__fit_intercept': (True, False),\n",
    "              'clf__refit': (True, False)}\n",
    "\n",
    "clf = GridSearchCV(clf, parameters, cv=10, n_jobs=-1)\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "get_metrics(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eg5rbAYi0xUV"
   },
   "outputs": [],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQCNtopA1ETe"
   },
   "outputs": [],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6YmcyM1M--A"
   },
   "source": [
    "[[20189   993]\n",
    " [  233  1763]]\n",
    "Test ROC AUC: 0.96438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1gsdEJsYbqC"
   },
   "source": [
    "**Multi-Layer Perceptron**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MLPClassifier, I used an iterative method to choose the best number of neurons for the network, using powers of two. The best result I found was with **32 neurons, although higher numbers showed similar results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "LQvyv_snT6Vw",
    "outputId": "75dde811-7103-4cd2-98bd-0677c66ae19c"
   },
   "outputs": [],
   "source": [
    "neuron_pow = 0\n",
    "\n",
    "# iterate until user cancels: take best number of neurons from results\n",
    "while True:\n",
    "    \n",
    "    print('MLP with {} neurons\\n'.format(2**neuron_pow))\n",
    "    \n",
    "    clf = Pipeline([\n",
    "        ('features', combined_features),\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=(2**neuron_pow), max_iter=200, \n",
    "                              activation='relu', random_state=12345, \n",
    "                              validation_fraction=0.1, verbose=True, early_stopping=True)),\n",
    "    ])\n",
    "\n",
    "    # parameters = {'clf__early_stopping': (True),\n",
    "    #               'clf__warm_start': (True),\n",
    "    #               'clf__solver': 'adam'}\n",
    "\n",
    "    # clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1)\n",
    "    clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "    get_metrics(clf)\n",
    "\n",
    "    neuron_pow+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zXZ_NPd0wwZ"
   },
   "outputs": [],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Zk8MaYz1DlQ"
   },
   "outputs": [],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bw4ZLmtPRMfS"
   },
   "source": [
    "32 Neurons\n",
    "\n",
    "[[20165  1040]\n",
    " [  257  1716]]\n",
    " \n",
    "Test Precision: 0.94130\n",
    "\n",
    "Test Recall: 0.94404\n",
    "\n",
    "Test F-Score: : 0.93994\n",
    "\n",
    "Test ROC AUC: 0.95322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9eB1ISBBMk_"
   },
   "source": [
    "**Multinomial Naive Bayes**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXCwYgzcAsZc"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('features', combined_features),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {'clf__fit_prior': (True, False)}\n",
    "\n",
    "clf = GridSearchCV(clf, parameters, cv=10, n_jobs=-1)\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "get_metrics(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afgU0hcL0uCU"
   },
   "outputs": [],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42sf5KkB086Z"
   },
   "outputs": [],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqFBydCzGvLY"
   },
   "source": [
    "[[20398  2146]\n",
    " [   24   610]]\n",
    " \n",
    "Test Precision: 0.91163\n",
    "\n",
    "Test Recall: 0.90638\n",
    "\n",
    "Test F-Score: : 0.87939\n",
    "\n",
    "Test ROC AUC: 0.86398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGNw1y4t0F1-"
   },
   "source": [
    "**Random Forest Classifier**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2fLkqbIBQl6"
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('features', combined_features),\n",
    "    ('clf', RandomForestClassifier(n_estimators=150, n_jobs=-1)),\n",
    "])\n",
    "\n",
    "parameters = {'clf__bootstrap': (True, False)}\n",
    "\n",
    "clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1)\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "get_metrics(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bmp7jaEN0CdC"
   },
   "outputs": [],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2xzvhLJ0CdI"
   },
   "outputs": [],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGmZ0aWjGxpE"
   },
   "source": [
    "[[20392  1938]\n",
    " [   30   818]]\n",
    " \n",
    "Test Precision: 0.91932\n",
    "\n",
    "Test Recall: 0.91509\n",
    "\n",
    "Test F-Score: : 0.89451\n",
    "\n",
    "Test ROC AUC: 0.94202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZ3pJQeBW-b5"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qBU4Bq2dBDn"
   },
   "source": [
    "**a. What are the text cleaning methods you tried? What are the ones you have included in the final code?**\n",
    "\n",
    "For my project, I tried several text cleaning methods. When I first received the assignment, I did some research into text processing for natural language processing experiments. I am also working on another project for another class where word embeddings are used to derive similarities between documents. It was great that these two assignments converged at some points!\n",
    "\n",
    "In my research, I learned about stopwords, spelling, alphabetic versus numeric characters, and how these features might impact the accuracy of a machine learning model. The following details some of the text processing methods I tried, most of which are included in the code for viewing, though some methods are commented out:\n",
    "\n",
    "One of the first text cleaning methods I tried was removing the stopwords from the `comment` column in the dataset. Stopwords are usually the most common words in a language, which can affect the meaning of phrases in a document. I used a package called the Natural Language Toolkit (nltk) that contains lists of stopwords. Using the imported list of stopwords, I removed them from each instance in `comment` using the Pandas method `apply`. Overall, I decided not to include my `remove_stopwords` function in my final code because it did not have a positive impact on my ROC AUC score for my models. I also used term frequency-inverse document frequency vectorizers (tfidf) later on in my experimentation, which removes stopwords when text mining - I will detail this later on in my findings.\n",
    "\n",
    "Another text cleaning method I tried was to correct the spelling of words in the `comment` attribute of the dataset. In my research, I learned that spelling can sometimes affect text mining. Sometimes, this effect is positive, in that misspelled words are spelled correctly. Other times, this effect can be negative, as misspelled and correctly spelled words are incorrectly spelled because the words do not exist in the dataset of correctly spelled words. I thought that maybe corrected spelling would 'normalize' so to say, some of the comments. After implementing a spellchecker function with `pyspellchecker`, I saw no improvement to the ROC AUC score, which surprised me, so I decided not to include the spellchecker in the final code.\n",
    "\n",
    "While implementing the above to functions, I built a parser method that applies specific transformations to the text as it is applied to each instance. Including the above two functions described, I also made all text lowercase and removed numbers and other nonalphabetical characters. \n",
    "\n",
    "Again, even with all of these functions used independently, I saw no increase in ROC AUC score. Why? Because the code provided by the strawman model includes a `TfidfTransformer` that automatically lowercases text and removes stopwords. *It pays to read the fine manual!*\n",
    "\n",
    "Here's what I did include: I read more into tfidf vectorizers and transformers and decided to replace the strawman `CountVectorizer` with a `TfidfVectorizer`. By doing so, I was able to better mine text for features using hyperparameter tuning. I also followed the professor's advice from the lecture and included a `FeatureUnion` of two `TfidfVectorizers`: one for words, another for characters. This significantly improved my ROC AUC score.\n",
    "\n",
    "***\n",
    "\n",
    "**b. What are the features you considered using? What features did you use in the final code?**\n",
    "\n",
    "For my project, I considered using several different types of feature extraction methods. First, I looked into fine-tuning the given strawman feature extraction methods, `CountVectorizer` and `TfidfTransformer`. Doing so, I saw an improvement in accuracy, albeit small. The results can be observed in my code above!\n",
    "\n",
    "Next, I used a `TfidfVectorizer` to combine the `CountVectorizer` and the `TfidfTransformer` above, per the professor's notes in the project assignment handout. I also used `GridSearchCV` to fine-tune this feature extraction method.\n",
    "\n",
    "Finally, I added `FeatureUnion` of two `TfidfVectorizer` methods. I have already detailed this implementation in part a. Ultimately, I settled on using the resulting `combined_features` variable for `FeatureUnion` in my model testing.\n",
    "\n",
    "I also considered using other features besides `comments`, such as `year`. `year` produced an ROC AUC of 0.95585, which was slightly worse than the strawman code. I also tried all of the other features in combination to `comment`; `logged_in`, `ns`, and `sample` negatively affected the AUC ROC score. I have since removed these features from the code.\n",
    "\n",
    "I believe that with the use of more efficient neural network packages like TensorFlow and Keras or implementation of a dataset of current events by year, these features might be able to provide some use. But for now, the other features do not have a significant positive effect on the ROC AUC. There is not enough about each of these other features to draw meaning right out of the bag. \n",
    "\n",
    "***\n",
    "\n",
    "**c. What optimizations did you add in your code, if any?**\n",
    "\n",
    "In my answers to these questions, I have detailed many optimizations that I made to the strawman code that had a significant effect on the metrics of my models. \n",
    "\n",
    "One of the most important optimizations was combining training and validation datasets to create a new and bigger training set. Doing so made my training dataset bigger and therefore, gave it more unseen data to train on. I was also able to still create validation sets using built-in Sci-kit learn methods, so it was not a problem to combine the two.\n",
    "\n",
    "Other optimizations I made include hyper-parameter tuning (discussed below), thoughtful analysis of the dataset to include specific feature extraction techniques (discussed above), and using functions to prevent code replication. \n",
    "\n",
    "My goal for this project was to learn the details of Sci-kit learn, and in doing so, learn how to clean data and optimize the machine learning models to accurately predict the labels for unseen data. I believe I was able to do so!\n",
    "***\n",
    "\n",
    "**d. What are the ML methods you tried out, and what were your best results with each method? Which was the best ML method you saw before tuning hyperparameters?**\n",
    "\n",
    "For this project, we were asked to try out three different machine learning methods. In order to truly understand the strawman code, however, I created multiple machine learning methods, far more than the professor asked. I did this so I could better understand the effects of parameterization and feature extraction on the existing code.\n",
    "\n",
    "For my first implementation, I chose to use the existing `LogisticRegression` machine learning model provided by the strawman code. Using the existing pipeline, I made alterations to the code: I combined validation and training sets to create a new training set with more instances that could then be used for validation using Sci-kit learn functions. One of the first functions I tried was `GridSearchCV`. Using this function *took a very long time to train*. With that in mind, I was careful to read the documentation for all of the implemented models to see what parameters *really* needed to be passed into `GridSearchCV`. \n",
    "\n",
    "Another model I tested was `LogisticRegression` with a `FeatureUnion` of two `TfidfVectorizers` as described in part a. This model also improved ROC AUC over the strawman code. Finally, I implemented `LogisticRegressionCV` which is a logistic regression model with cross-validation. This model was even better than the previous two!\n",
    "\n",
    "*For my three new machine learning models, I tested `MLPClassifier`, `MultiNomialNaiveBayes`, and a `RandomForestClassifier`.*\n",
    "\n",
    "I have been working with neural networks in another class so I was excited to see how Sci-kit learn's multi-layer perceptron would perform. To my surprise, I was not as fast as I expected it to be. I have been using TensorFlow and Keras neural networks and was blown away by how much slower the Sci-kit learn implementation was. Nonetheless, I implemented the `MLPClassifier` after I had figured out text mining methods and `GridSearchCV`. I was able to train the neural network with a decent ROC AUC score. \n",
    "\n",
    "As soon as I implemented the `MultiNomialNaiveBayes` classifier and got the ROC AUC score, I knew it was a bad performer for this dataset. There are not many parameters for this model, and even after using `GridSearchCV`, the score did not improve significantly.\n",
    "\n",
    "Finally, my `RandomForestClassifier` also performed pretty well given the dataset. Again, I used `GridSearchCV` to fine-tune the model. Overall, I think that the `RandomForestClassifier` performed pretty well!\n",
    "\n",
    "***\n",
    "\n",
    "**e. What hyper-parameter tuning did you do, and by how many percentage points did your accuracy go up?**\n",
    "\n",
    "I used several different techniques for hyper-parameter tuning. The most important technique I used was to read the fine manual for all of the models. Reading the documentation was critical in making decisions about which hyper-parameters to choose for each model. For example, gradient descent is great for small datasets, but not for large ones. Here, stochastic gradient descent would be more applicable. Therefore, hyper-parameters should only include stochastic gradient descent, as our dataset is quite large. After handpicking, I used `GridSearchCV` to tune the hyper-parameters, passing in the values that would have the best effect on the models, as discussed above. \n",
    "\n",
    "My accuracy increased ~1.5-2.0 percentage points after hyperparameter tuning.\n",
    "\n",
    "***\n",
    "\n",
    "**f. What did you learn from the different metrics? Did you try cross-validation?**\n",
    "\n",
    "I used a `get_metrics` function to keep track of all of my metrics for each experiment. As the project required, I implemented a confusion matrix, precision, recall, f-score, and ROC AUC. \n",
    "\n",
    "From these metrics I was able to learn more about the accuracy of my predictions for each of the classes, `attack == 1` and `attack == 0`. I thought it was great to see that some models more accurately predict positive classes than negative classes and vice versa. As part of the assignment, I used ROC AUC as the primary score metric and focused on increasing this specific metric. \n",
    "\n",
    "In my project, I did use cross-validation in several models. The first instance is with `GridSearchCV` and the strawman code. Second, I used a `LogisticRegressionCV` model. I also used cross-validation in my `MLPClassifier`. \n",
    "\n",
    "I think it is important to note that I combined the `dev` set from the Wikipedia dataset with the `train` set, and then created validation sets from that total, per the professor's advice during the Tuesday discussions.\n",
    "\n",
    "***\n",
    "\n",
    "**g. What are your best final Result Metrics? By how much is it better than the strawman figure? Which model gave you this performance?**\n",
    "\n",
    "My final result metrics are as follows:\n",
    "\n",
    "`LogisticRegression` (strawman):\n",
    "[[20280  1236]\n",
    " [  142  1520]],\n",
    "Test Precision: 0.93923,\n",
    "Test Recall: 0.94055,\n",
    "Test F-Score: : 0.93396,\n",
    "Test ROC AUC: 0.95697\n",
    "\n",
    "`LogisticRegressionCV`\n",
    "[[20204  1014]\n",
    " [  218  1742]],\n",
    "Test Precision: 0.94467,\n",
    "Test Recall: 0.94685,\n",
    "Test F-Score: : 0.94287,\n",
    "Test ROC AUC: 0.96361\n",
    "\n",
    "`MLPClassifier`\n",
    "[[20165  1040]\n",
    " [  257  1716]],\n",
    "Test Precision: 0.94130,\n",
    "Test Recall: 0.94404,\n",
    "Test F-Score: : 0.93994,\n",
    "Test ROC AUC: 0.95322\n",
    "\n",
    "`MultiNomialNB`\n",
    "[[20398  2146]\n",
    " [   24   610]],\n",
    "Test Precision: 0.91163,\n",
    "Test Recall: 0.90638,\n",
    "Test F-Score: : 0.87939,\n",
    "Test ROC AUC: 0.86398\n",
    "\n",
    "`RandomForestClassifier`\n",
    "[[20392  1938]\n",
    " [   30   818]],\n",
    "Test Precision: 0.91932,\n",
    "Test Recall: 0.91509,\n",
    "Test F-Score: : 0.89451,\n",
    "Test ROC AUC: 0.94202\n",
    "\n",
    "***\n",
    "\n",
    "**h. What is the most interesting thing you learned from doing the report?**\n",
    "\n",
    "The most interesting thing I learned was text processing methods. \n",
    "\n",
    "In a close second, I think that it is worthy to mention sci-ki learn. I have used the package before, but not in the level of detail that we used for this project. I thought it was exciting to learn how everything worked and feel that I can and will be able to accomplish a lot with this package. I am learning TensorFlow and Keras in another class at the moment, so it is great to learn more about a similar package.\n",
    "\n",
    "***\n",
    "\n",
    "**i. What was the hardest thing to do?**\n",
    "\n",
    "The hardest thing to do was waiting for the models to train. \n",
    "\n",
    "I consider myself a very patient person and had no trouble waiting for the models to complete training. However, I realized that unlike previous classes where I sat and watched the program compile in a short amount of time...with machine learning, doing that is a waste of time. \n",
    "\n",
    "The hardest thing to do with this project was to manage my time more efficiently. I realized that training could happen in the background of accomplishing other tasks, writing documentation, writing the answers to some of these questions, doing homework for other classes. In the beginning, I spent a lot of time waiting for models to be trained and treated that time as a break. This was an important lesson to learn and I am glad that I picked up on it early enough."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "nathaniel_haddad_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
